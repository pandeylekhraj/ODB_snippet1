{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814a83ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002080b",
   "metadata": {},
   "source": [
    "# function reading the json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451d3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "   \n",
    "def save_json_to_file(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "028617cb-aeff-48e5-b1fd-c72b8fecba14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON loaded successfully.\n",
      "JSON loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess JSON data\n",
    "def preprocess_json(data):\n",
    "    # Fix unescaped backslashes\n",
    "    data = re.sub(r'(?<!\\\\)\\\\\\\\', '\\\\\\\\', data)  # Correctly escape backslashes\n",
    "\n",
    "    # Further custom processing to fix common errors\n",
    "    return data\n",
    "def load_and_clean_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Assuming the second line contains JSON\n",
    "    json_data = lines[1].strip()\n",
    "\n",
    "    # Preprocess the JSON data before loading\n",
    "    json_data = preprocess_json(json_data)\n",
    "\n",
    "    try:\n",
    "        # Attempt to parse the JSON data\n",
    "        data = json.loads(json_data)\n",
    "        print(\"JSON loaded successfully.\")\n",
    "        return data\n",
    "    except json.decoder.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError: {e}\")\n",
    "        print(\"Error occurred at position:\", e.pos)\n",
    "        # Optionally print a snippet of the data around the error\n",
    "        print(\"Data snippet around error:\")\n",
    "        print(json_data[e.pos - 50:e.pos + 50])  # Print a portion around the error position\n",
    "        return None\n",
    "        \n",
    "json2 = load_and_clean_json('database_newend.txt')\n",
    "json1 = load_and_clean_json('database_newstart.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3ea86-5486-43ab-8f89-9b66553a2fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7e2a0ea",
   "metadata": {},
   "source": [
    "# removing the Keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f9f7f77-bf33-44b9-a348-d4f09d6e3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_json_remove_slash_key_value(data, pattern=\"/key\"):\n",
    "    \"\"\"\n",
    "    Recursively removes keys from a dictionary if they contain the given pattern.\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(data, dict):\n",
    "        return data  # If data is not a dictionary, return as is\n",
    "    \n",
    "    new_dict = {}\n",
    "    for k, v in data.items():\n",
    "        if pattern not in k:  # Exclude keys that contain the pattern\n",
    "            if isinstance(v, dict):\n",
    "                new_dict[k] = filter_json_remove_slash_key_value(v, pattern)  # Recurse for nested dict\n",
    "            else:\n",
    "                new_dict[k] = v  # Keep the value as is\n",
    "    \n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38bd5c2d-25c3-45e5-b5c3-96089e1b6f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d831ee23-bb8f-41ad-ac2f-5944fb379aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_to_file(json2, '24250227_135126_end_original.json')\n",
    "save_json_to_file(json1, '24250227_135126_start_original.json')\n",
    "json2=filter_json_remove_slash_key_value(json2)\n",
    "json1=filter_json_remove_slash_key_value(json1)\n",
    "save_json_to_file(json2, '24250227_135126_end_remove_slash_key_value.json')\n",
    "save_json_to_file(json1, '24250227_135126_start_remove_slash_key_value.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a32e2-70b9-411c-86bf-6d385a5e62ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbcfa23-9a14-47ad-8bf5-3bf444704ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da209de-c22b-41a1-91af-f81a36c94501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23b3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_json_remove_slash_key(data, pattern=\"/key\"):\n",
    "#     # Create a new dictionary to store the filtered results\n",
    "#     new_dict = {}\n",
    "    \n",
    "#     for k, v in data.items():\n",
    "#         # If the key does not contain the pattern, add it to the new dictionary\n",
    "#         if pattern not in k:\n",
    "#             if isinstance(v, dict):\n",
    "#                 # Recursively apply the function if the value is a dictionary\n",
    "#                 new_dict[k] = filter_json_remove_slash_key(v, pattern)\n",
    "#             else:\n",
    "#                 # Otherwise, just add the key-value pair\n",
    "#                 new_dict[k] = v\n",
    "    \n",
    "#     return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "214c2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_json_remove_system_keys(data):\n",
    "    # Ensure we only modify the 'System' key if it exists\n",
    "    if 'System' in data:\n",
    "        # List of keys to remove from 'System'\n",
    "        keys_to_remove = ['Buffers', 'Transition', 'Flush', 'Tmp']\n",
    "        \n",
    "        # Loop through the list and remove each key from 'System' if it exists\n",
    "        for key in keys_to_remove:\n",
    "            data['System'].pop(key, None)\n",
    "    # Filter out entries that have \"last_written\" in the key\n",
    "    if isinstance(data, list):  # Check if data is a list of dictionaries\n",
    "        data = [entry for entry in data if \"/last_written\" not in entry.get(\"key\", \"\")]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16862bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_json_remove_legacytrigger(data):\n",
    "    \"\"\"\n",
    "    This function recursively removes keys that match the pattern \"/Detectors/DetXX/Settings/LegacyTrigger\".\n",
    "    \"\"\"\n",
    "    def recursive_trim(d):\n",
    "        if isinstance(d, dict):\n",
    "            # Identify keys matching the specific pattern\n",
    "            keys_to_remove = [key for key in d if \"/Detectors/Det\" in key and \"/Settings/LegacyTrigger\" in key]\n",
    "            # Remove the matching keys\n",
    "            for key in keys_to_remove:\n",
    "                del d[key]\n",
    "            # Recursively process nested dictionaries or lists\n",
    "            for key, value in d.items():\n",
    "                if isinstance(value, (dict, list)):  # If the value is a dictionary or list, apply recursion\n",
    "                    recursive_trim(value)\n",
    "\n",
    "    # Call the recursive trim function on the data\n",
    "    recursive_trim(data)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9e840-9e1b-4808-ab75-b72c721780ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e78cdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing last_readout duplicate time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6edcef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_data_remove_duplicate_last_written1(data):\n",
    "#     seen_values = set()  # Stores unique \"last_written\" values\n",
    "\n",
    "#     def recurse(d):\n",
    "#         if isinstance(d, dict):\n",
    "#             new_dict = {}\n",
    "#             for k, v in d.items():\n",
    "#                 if isinstance(v, dict) and \"last_written\" in v:\n",
    "#                     last_written_val = v[\"last_written\"]\n",
    "#                     if last_written_val not in seen_values:\n",
    "#                         seen_values.add(last_written_val)\n",
    "#                         new_dict[k] = v  # Keep the first occurrence\n",
    "#                 else:\n",
    "#                     # Recursively process nested dictionaries and lists\n",
    "#                     new_dict[k] = recurse(v)\n",
    "#             return new_dict\n",
    "#         elif isinstance(d, list):\n",
    "#             return [recurse(item) for item in d]\n",
    "#         return d\n",
    "\n",
    "#     # First pass to collect duplicates\n",
    "#     filtered_data = recurse(data)\n",
    "\n",
    "#     return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36575e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json2 = read_json_from_file('savedata.json')\n",
    "\n",
    "# json2 = filter_json_remove_slash_key(json2)\n",
    "\n",
    "\n",
    "# json2 = filter_json_remove_system_keys(json2)\n",
    "\n",
    "\n",
    "# json2 = filter_json_remove_legacytrigger(json2)\n",
    "\n",
    "\n",
    "# json2 = json_filter_remove_last_written(json2)\n",
    "# save_json_to_file(json2, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc068403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to recursively remove keys ending with \"last_written\"\n",
    "# def json_filter_remove_last_written(data):\n",
    "#     if isinstance(data, dict):\n",
    "#         return {k: json_filter_remove_last_written(v) for k, v in data.items() if not k.endswith(\"last_written\")}\n",
    "#     elif isinstance(data, list):\n",
    "#         return [json_filter_remove_last_written(item) for item in data]\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d620202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_json_differ_Readback_Setting(d):\n",
    "    \"\"\"\n",
    "    Filters out keys from the 'Settings' section of Detectors/DetXX if they match the values in the 'Readback' section.\n",
    "    Keeps the keys that differ between the 'Settings' and 'Readback' paths.\n",
    "    \"\"\"\n",
    "    def recurse(d, readback_data, settings_data, path=\"\"):\n",
    "        \"\"\"\n",
    "        Recursive function to traverse and compare the two paths.\n",
    "        - d: the current data (settings or readback)\n",
    "        - readback_data: the current data from the Readback section\n",
    "        - settings_data: the current data from the Settings section\n",
    "        - path: the current path to compare (for debugging or tracking)\n",
    "        \"\"\"\n",
    "        if isinstance(d, dict):\n",
    "            # Iterate through the dictionary\n",
    "            new_dict = {}\n",
    "            for k, v in d.items():\n",
    "                new_path = f\"{path}/{k}\" if path else k\n",
    "                \n",
    "                # Check if the path is within the Readback or Settings section\n",
    "                if re.match(r\"/Detectors/Det\\d+/Readback\", new_path):\n",
    "                    # If the path starts with Readback, compare values from the 'Settings' section\n",
    "                    if new_path in settings_data and settings_data[new_path] == v:\n",
    "                        # If values are the same, remove them from the Settings data\n",
    "                        settings_data.pop(new_path, None)\n",
    "                    else:\n",
    "                        new_dict[k] = recurse(v, readback_data, settings_data, new_path)\n",
    "                elif re.match(r\"/Detectors/Det\\d+/Settings\", new_path):\n",
    "                    # Handle the Settings path, keep values only if different from Readback\n",
    "                    if new_path in readback_data and readback_data.get(new_path) != v:\n",
    "                        new_dict[k] = v\n",
    "                    else:\n",
    "                        # Remove from settings if matched with readback\n",
    "                        settings_data.pop(new_path, None)\n",
    "                else:\n",
    "                    new_dict[k] = recurse(v, readback_data, settings_data, new_path)\n",
    "            return new_dict\n",
    "        elif isinstance(d, list):\n",
    "            return [recurse(item, readback_data, settings_data, f\"{path}[{idx}]\") for idx, item in enumerate(d)]\n",
    "        return d\n",
    "\n",
    "    # Extract readback and settings data from the main structure\n",
    "    readback_data = d.get('Detectors', {}).get('DetXX', {}).get('Readback', {})\n",
    "    settings_data = d.get('Detectors', {}).get('DetXX', {}).get('Settings', {})\n",
    "\n",
    "    # Filter out the matching values and keep differing ones\n",
    "    filtered_data = recurse(d, readback_data, settings_data)\n",
    "    #print(filtered_data)\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a2c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_differences(start_obj, end_obj, parent_key='', differences=None):\n",
    "#     if differences is None:\n",
    "#         differences = {}\n",
    "\n",
    "#     # Get all the unique keys from both objects\n",
    "#     all_keys = set(start_obj.keys()).union(set(end_obj.keys()))\n",
    "\n",
    "#     # Iterate through all keys\n",
    "#     for key in all_keys:\n",
    "#         # Build the current key's path\n",
    "#         current_key = parent_key + key\n",
    "\n",
    "#         # Case where the values are lists\n",
    "#         if isinstance(start_obj.get(key), list) or isinstance(end_obj.get(key), list):\n",
    "#             # Compare lists by joining them into a string for easy comparison\n",
    "#             if str(start_obj.get(key)) != str(end_obj.get(key)):\n",
    "#                 differences[current_key] = end_obj.get(key)\n",
    "\n",
    "#         # Case where the values are dictionaries\n",
    "#         elif isinstance(start_obj.get(key), dict) and isinstance(end_obj.get(key), dict):\n",
    "#             # Recurse into the nested dictionaries with updated parent_key\n",
    "#             find_differences(start_obj.get(key), end_obj.get(key), current_key + '/', differences)\n",
    "\n",
    "#         # Case where the values are primitive types (strings, numbers, etc.)\n",
    "#         elif start_obj.get(key) != end_obj.get(key):\n",
    "#             differences[current_key] = end_obj.get(key)\n",
    "\n",
    "#     return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fcaff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_differences(start_obj, end_obj):\n",
    "    def recursive_diff(start, end):\n",
    "        diff = {}\n",
    "        for key in end.keys():\n",
    "            if key not in start:\n",
    "                # Key is new in end_obj\n",
    "                diff[key] = end[key]\n",
    "            elif isinstance(start[key], dict) and isinstance(end[key], dict):\n",
    "                # Recursively check nested dictionaries\n",
    "                nested_diff = recursive_diff(start[key], end[key])\n",
    "                if nested_diff:  # Only add if there are differences\n",
    "                    diff[key] = nested_diff\n",
    "            elif isinstance(start[key], list) and isinstance(end[key], list):\n",
    "                # Compare lists directly\n",
    "                if start[key] != end[key]:\n",
    "                    diff[key] = end[key]\n",
    "            elif start[key] != end[key]:\n",
    "                # Value has changed\n",
    "                diff[key] = end[key]\n",
    "        return diff\n",
    "\n",
    "    return recursive_diff(start_obj, end_obj)\n",
    "# differences = find_differences(start_json, end_json)\n",
    "# print(differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8716ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "json2 = read_json_from_file('24250227_135126_end_original.json')\n",
    "json2 = filter_json_remove_slash_key_value(json2)\n",
    "json2 = filter_json_remove_system_keys(json2)\n",
    "json2 = filter_json_remove_legacytrigger(json2)\n",
    "# json2 = filter_json_differ_Readback_Setting(json2)\n",
    "json1 = read_json_from_file('24250227_135126_start_original.json')\n",
    "json1 = filter_json_remove_slash_key_value(json1)\n",
    "json1 = filter_json_remove_system_keys(json1)\n",
    "# json1 = filter_json_differ_Readback_Setting(json1)\n",
    "finddifference=find_differences(json2,json1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9743f39-fd56-4fed-9902-f55c3c7445e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logger': {'Channels': {'0': {'Settings': {'Current filename': '.RUN00075_DUMP0001.mid.gz'},\n",
       "    'Statistics': {'Disk level': 0.001500701734860943,\n",
       "     'Bytes written': 4489610,\n",
       "     'Files written': 20818,\n",
       "     'Events written': 361,\n",
       "     'Bytes written total': 17319883542959.0,\n",
       "     'Bytes written subrun': 2244248,\n",
       "     'Bytes written uncompressed': 69735101}}}},\n",
       " 'System': {'Clients': {'2049309': {'Run state': 1},\n",
       "   '2049311': {'Run state': 1}}},\n",
       " 'Runinfo': {'Stop time': 'Thu Feb 27 13:53:03 2025',\n",
       "  'Stop time binary': '0x67c0b48f',\n",
       "  'Transition in progress': 0},\n",
       " 'Equipment': {'EBlvl': {'Statistics': {'Events sent': 56}},\n",
       "  'EBuilder': {'Statistics': {'Events sent': 355},\n",
       "   'Special dump numbers': {'Last BORR': 1, 'Last BORTS': 1}},\n",
       "  'L2Trigger': {'Statistics': {'Events sent': 86}},\n",
       "  'readoutfe_SDU': {'Statistics': {'Events sent': 0}},\n",
       "  'readoutveto03': {'Statistics': {'Events sent': 10}},\n",
       "  'L2TriggerHistory': {'Statistics': {'Events sent': 10}},\n",
       "  'readouthistory03': {'Statistics': {'Events sent': 11}},\n",
       "  'readoutSDUhistory': {'Statistics': {'Events sent': 11}},\n",
       "  'Thermometry_RevF03': {'Statistics': {'Events sent': 2}}}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json2 = read_json_from_file('24250227_135126_end_original.json')\n",
    "json2 = filter_json_remove_slash_key_value(json2)\n",
    "json2 = filter_json_remove_system_keys(json2)\n",
    "json2 = filter_json_remove_legacytrigger(json2)\n",
    "# json2 = filter_json_differ_Readback_Setting(json2)\n",
    "json1 = read_json_from_file('24250227_135126_start_original.json')\n",
    "json1 = filter_json_remove_slash_key_value(json1)\n",
    "json1 = filter_json_remove_system_keys(json1)\n",
    "finddifference=find_differences(json1,json2)\n",
    "finddifference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "545af90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logger': {'Channels': {'0': {'Settings': {'Current filename': '.RUN00075_DUMP0000.mid.gz'},\n",
       "    'Statistics': {'Disk level': 0.0015000163269089928,\n",
       "     'Bytes written': 1048576,\n",
       "     'Files written': 20816,\n",
       "     'Events written': 1,\n",
       "     'Bytes written total': 17319880101925.0,\n",
       "     'Bytes written subrun': 1048576,\n",
       "     'Bytes written uncompressed': 17422558}}}},\n",
       " 'System': {'Clients': {'2049309': {'Run state': 0},\n",
       "   '2049311': {'Run state': 0}}},\n",
       " 'Runinfo': {'Stop time': 'Fri Feb 21 15:18:30 2025',\n",
       "  'Stop time binary': '0x00000000',\n",
       "  'Transition in progress': 1},\n",
       " 'Equipment': {'EBlvl': {'Statistics': {'Events sent': 28}},\n",
       "  'EBuilder': {'Statistics': {'Events sent': 143},\n",
       "   'Special dump numbers': {'Last BORR': -1, 'Last BORTS': -1}},\n",
       "  'L2Trigger': {'Statistics': {'Events sent': 33}},\n",
       "  'readoutfe_SDU': {'Statistics': {'Events sent': 110}},\n",
       "  'readoutveto03': {'Statistics': {'Events sent': 0}},\n",
       "  'L2TriggerHistory': {'Statistics': {'Events sent': 5}},\n",
       "  'readouthistory03': {'Statistics': {'Events sent': 0}},\n",
       "  'readoutSDUhistory': {'Statistics': {'Events sent': 5}},\n",
       "  'Thermometry_RevF03': {'Statistics': {'Events sent': 0}}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finddifference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json2 = filter_json_remove_slash_key(json2)\n",
    "# json2 = filter_json_remove_system_keys(json2)\n",
    "# json2 = filter_json_remove_legacytrigger(json2)\n",
    "# json2 =filter_remove_last_written(json2)\n",
    "# json2 = filter_json_differ_Readback_Setting(json2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f64c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Disk space its memory not relevant\n",
    "# json2 = read_json_from_file('savedata1.json')\n",
    "# size_in_bytes_comprehensive = asizeof.asizeof(json2)\n",
    "# size_in_mb = size_in_bytes_comprehensive / (1024)\n",
    "# # Print size in MB with 3 decimal points\n",
    "# print(f\"Size of json2 : {size_in_mb:.2f} KB\")\n",
    "\n",
    "# json2 = filter_json_remove_slash_key(json2)\n",
    "# size_in_bytes_comprehensive = asizeof.asizeof(json2)\n",
    "# size_in_mb = size_in_bytes_comprehensive / (1024)\n",
    "# print(f\"Size of json2 remove /Key : {size_in_mb:.2f} KB\")\n",
    "\n",
    "# json2 = filter_json_remove_system_keys(json2)\n",
    "# size_in_bytes_comprehensive = asizeof.asizeof(json2)\n",
    "# size_in_mb = size_in_bytes_comprehensive / (1024)\n",
    "# print(f\"Size of json2 remove system expect client: {size_in_mb:.2f} KB\")\n",
    "\n",
    "# json2 = filter_json_remove_legacytrigger(json2)\n",
    "# size_in_bytes_comprehensive = asizeof.asizeof(json2)\n",
    "# size_in_mb = size_in_bytes_comprehensive / (1024)\n",
    "# print(f\"Size of json2 remove legacytrigger: {size_in_mb:.2f} KB\")\n",
    "\n",
    "# json2 =json_filter_remove_last_written(json2)\n",
    "# size_in_bytes_comprehensive = asizeof.asizeof(json2)\n",
    "# size_in_mb = size_in_bytes_comprehensive / (1024)\n",
    "# print(f\"Size of json2 remove last written: {size_in_mb:.2f} KB\")\n",
    "\n",
    "# # either way is same remove  not needed\n",
    "# json2 = filter_json_differ_Readback_Setting(json2)\n",
    "# size_in_bytes_comprehensive = asizeof.asizeof(json2)\n",
    "# size_in_mb = size_in_bytes_comprehensive / (1024)\n",
    "# print(f\"Size of json2 remove settings if Readback is same as Detectors/DetXX: {size_in_mb:.2f} KB\")\n",
    "\n",
    "# # either way is same remove \n",
    "# size_in_bytes_comprehensive = asizeof.asizeof(finddifference3)\n",
    "# size_in_mb = size_in_bytes_comprehensive / (1024)\n",
    "# print(f\"Size of json2 remove if start is same as stop: {size_in_mb:.2f} KB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e998be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving Json\n"
     ]
    }
   ],
   "source": [
    "json2 = read_json_from_file('24250227_135126_end_original.json')\n",
    "json2 = filter_json_remove_slash_key_value(json2)\n",
    "save_json_to_file(json2, \"24250227_135126_end_remove_slash_key_value.json\")\n",
    "json2 = filter_json_remove_system_keys(json2)\n",
    "save_json_to_file(json2, \"24250227_135126_end_remove_system_keys_except_clients.json\")\n",
    "\n",
    "json2 = filter_json_remove_legacytrigger(json2)\n",
    "save_json_to_file(json2, \"24250227_135126_end_remove_legacytrigger.json\")\n",
    "\n",
    "save_json_to_file(finddifference, \"24250227_135126_diff_start_end_ODB.json\")\n",
    "\n",
    "print(\"saving Json\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dea7e04-0334-4154-8d05-e3760523eadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving start Json\n"
     ]
    }
   ],
   "source": [
    "json2 = read_json_from_file('24250227_135126_start_original.json')\n",
    "json2 = filter_json_remove_slash_key_value(json2)\n",
    "save_json_to_file(json2, \"24250227_135126_start_remove_slash_key_value.json\")\n",
    "json2 = filter_json_remove_system_keys(json2)\n",
    "save_json_to_file(json2, \"24250227_135126_start_remove_system_keys_except_clients.json\")\n",
    "\n",
    "json2 = filter_json_remove_legacytrigger(json2)\n",
    "save_json_to_file(json2, \"24250227_135126_start_remove_legacytrigger.json\")\n",
    "save_json_to_file(finddifference, \"24250227_135126_diff_end_withstart_ODB.json\")\n",
    "print(\"saving start Json\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7f4ec0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Action (odb end json for series=24250227_135126)        Disk Size (KB)\n",
      "===========================================================================\n",
      "Original_json                                                 11509.07\n",
      "Json_removing_slash_key_values                                 7612.37\n",
      "Json_removing_system_keys_except_clients                       7300.08\n",
      "Json_removing_legacytrigger                                    7300.08\n",
      "Json_difference_in_end_ODB_with_start                             7.77\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define file names corresponding to each step with sizes in KB\n",
    "file_sizes_on_disk = {\n",
    "    \"Original_json\": 11785291/1024,\n",
    "    \"Json_removing_slash_key_values\": 7795067 / 1024,\n",
    "    \"Json_removing_system_keys_except_clients\": 7475285 / 1024,\n",
    "    \"Json_removing_legacytrigger\": 7475285 / 1024,\n",
    "    \"Json_difference_in_end_ODB_with_start\": 7955/1024\n",
    "}\n",
    "print()\n",
    "# Print table header with better formatting\n",
    "print(f\"{'Action (odb end json for series=24250227_135126)':<50}{'Disk Size (KB)':>20}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# Print each file's disk size in a neatly formatted way\n",
    "for file_name, size_kb in file_sizes_on_disk.items():\n",
    "    print(f\"{file_name:<50}{size_kb:>20.2f}\")\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e6eccdc-533b-4428-bf63-10ed3ab3eb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Action (odb start json for series=24250227_135126)      Disk Size (KB)\n",
      "===========================================================================\n",
      "Original_json                                                  4233.97\n",
      "Json_removing_slash_key_values                                 7612.41\n",
      "Json_removing_system_keys_except_clients                       7300.08\n",
      "Json_removing_legacytrigger                                    7300.08\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define file names corresponding to each step with sizes in KB\n",
    "file_sizes_on_disk = {\n",
    "    \"Original_json\": 4335590/1024,\n",
    "    \"Json_removing_slash_key_values\": 7795110 / 1024,\n",
    "    \"Json_removing_system_keys_except_clients\": 7475284 / 1024,\n",
    "    \"Json_removing_legacytrigger\": 7475284 / 1024,\n",
    "    \n",
    "}\n",
    "print()\n",
    "# Print table header with better formatting\n",
    "print(f\"{'Action (odb start json for series=24250227_135126)':<50}{'Disk Size (KB)':>20}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# Print each file's disk size in a neatly formatted way\n",
    "for file_name, size_kb in file_sizes_on_disk.items():\n",
    "    print(f\"{file_name:<50}{size_kb:>20.2f}\")\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71b8a85-a02f-4ee9-b7ae-301aff32a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON error at line 1, column 4266656: Expecting ',' delimiter\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_json_with_error_handling(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "            return json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON error at line {e.lineno}, column {e.colno}: {e.msg}\")\n",
    "        return None  # Or handle it as needed\n",
    "read_json_with_error_handling('database_newend.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd39f6-8ede-47fd-a0d7-4dd940943bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7507d5-e9b7-42a6-978d-0d2624ffbb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f56a5858-f0f5-4140-8771-115974198a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_json_compact(data, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, separators=(\",\", \":\"))  # Compact format (no extra spaces)\n",
    "\n",
    "# Load, filter, and save the compact file\n",
    "def clean_json_file(input_file, output_file, pattern=\"/key\"):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)  \n",
    "\n",
    "    filtered_data = filter_json_remove_slash_key_value(data, pattern)  \n",
    "\n",
    "    save_json_compact(filtered_data, output_file)  # Save without spaces\n",
    "\n",
    "# Example Usage\n",
    "clean_json_file(\"abscend.json\", \"abscend_new.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1252c-e5f7-455b-a2ed-0525b35cf7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
